{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking.com Feature Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the file BookingDotCom_HotelReviews.xlsx from Canvas. This file contains over 515,000 guest reviews and rating of almost 1500 hotels across Europe scraped from popular hotel reservation website Booking.com. The text data was cleaned by removing unicode and punctuation and transformed to lower case. No other preprocessing was done. More information on each field is provided in the \"Data Description\" tab of the Excel file.\n",
    "\n",
    "        1. What are the top five hotel features (e.g., location, staff, etc.) that customers mention the most in positive reviews and top five features they mention most in negative reviews? Your identified features must make sense (e.g., \"great\" or \"negative\" are not features). (3 points)\n",
    "        \n",
    "        2. What are the top five features that customers prefer most if they are a solo traveler vs traveling with a group vs on a business trip vs a leisure trip vs traveling as a couple vs a family with young children. You will find these categories in the \"Tags\" column. There are a few more tags that we don't need. (2 points).\n",
    "\n",
    "        3. What are the top five features customers like most and top five features they complain about most about hotels in United Kingdom, France, Italy, and Spain? Country information is available inside Hotel_Address. (2 points)\n",
    "        \n",
    "        4. Create a dashboard with the following plots; (1) \"Top Five Hotels Overall\" with consistently high ratings, (2) Bottom Five Hotels Overall\" with consistently low ratings, (3) Five Most Improved Hotels\" with the highest improvement in average ratings from 2015 to 2017, showing their average ratings for each of the three years. (0.5+0.5+2 points).\n",
    "\n",
    "Write clear, compact, and understandable code with comment/markdown statements as appropriate. Non-working code or unnecessary code will be penalized. \n",
    "\n",
    "Submit your Jupyter file using the link below or provide a link to your Google Colab or Github file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages to use\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# load dataframe\n",
    "df = pd.read_excel(\"BookingDotCom_HotelReviews.xlsx\", sheet_name=\"Data\")\n",
    "\n",
    "# sample the first 100 rows of df\n",
    "df = df[:100]\n",
    "\n",
    "# rename df columns to lower case\n",
    "df.columns= df.columns.str.lower()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bookingdotcom.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase & remove double/unneccessary spaces between words in columns of comments\n",
    "df['positive_comments'] = df['positive_comments'].str.lower().apply(lambda x: \" \".join(x.split()))\n",
    "df['negative_comments'] = df['negative_comments'].str.lower().apply(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['positive_comments'] = df['positive_comments'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df['negative_comments'] = df['negative_comments'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echemochek\\AppData\\Local\\Temp\\ipykernel_16728\\3288369858.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['positive_comments'] = df['positive_comments'].str.replace('\\d+', '')\n",
      "C:\\Users\\echemochek\\AppData\\Local\\Temp\\ipykernel_16728\\3288369858.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['negative_comments'] = df['negative_comments'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "# drop numbers from string\n",
    "\n",
    "df['positive_comments'] = df['positive_comments'].str.replace('\\d+', '')\n",
    "df['negative_comments'] = df['negative_comments'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create column with country name; we get the list of countries from the pycountry package\n",
    "and use the hotel_address column to extract country name\n",
    "'''\n",
    "\n",
    "df['country'] = df[\"hotel_address\"].apply(\n",
    "    lambda address: ' '.join([c.name for c in pycountry.countries if c.name in address])\n",
    "    )\n",
    "\n",
    "# use the review_date column to extract the year and store in new column\n",
    "df['year'] = pd.DatetimeIndex(df['review_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this step we deal with the tags column using the steps defined below:\n",
    "    1. Define tags we are interested in\n",
    "    2. Define a function to apply to tags column to remove tags we are not interested in by:\n",
    "            - Converting the individual row values to list (from string) e.g. \"[' Leisure trip ']\" -> [' leisure trip ']\n",
    "            - Strip the whitespaces from individual elements e.g. [' leisure trip ']-> ['leisure trip']\n",
    "            - Drop tags we are not interested in\n",
    "'''\n",
    "\n",
    "# customer tags we are interested in\n",
    "customer_tags = [\n",
    "    'solo traveler',\n",
    "    'group',\n",
    "    'business trip',\n",
    "    'leisure trip',\n",
    "    'couple',\n",
    "    'family with young children'\n",
    "]\n",
    "\n",
    "def clean_tag(x):\n",
    "\n",
    "    # convert value from string to a list\n",
    "    import ast\n",
    "    \n",
    "    myTags = ast.literal_eval(x.lower())\n",
    "\n",
    "    # strip whitespaces from elements and drop those we are not interested in\n",
    "    myTags = [customerTag.strip() for customerTag in myTags if customerTag.strip() in customer_tags]\n",
    "\n",
    "    return myTags\n",
    "\n",
    "\n",
    "# apply clean_tag() function to the tags column\n",
    "df['tags'] = df['tags'].apply(lambda x: clean_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function applied to tags column to extract new columns for customer categories\n",
    "Lambda function will be used as in the steps below\n",
    "'''\n",
    "def split_tag(x:list, tagName:str) -> int:\n",
    "    \n",
    "    t = [1 if tagName in x else 0][0]\n",
    "    \n",
    "    return t\n",
    "\n",
    "# dictionary of column names (new additional columns) and customer tags (as contained in tags of interest)\n",
    "tagDict = {\n",
    "    'solo_traveler' : 'solo traveler',\n",
    "    'group' : 'group',\n",
    "    'business_trip' : 'business trip',\n",
    "    'leisure_trip' : 'leisure trip',\n",
    "    'couple' : 'couple',\n",
    "    'family_with_young_children' : 'family with young children'\n",
    "}\n",
    "\n",
    "# applying function on tags column to get new separated columns\n",
    "for key, value in tagDict.items():\n",
    "    df[key] = df['tags'].apply(lambda x: split_tag(x, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we do not need for now\n",
    "colsToDrop = [\n",
    "    'hotel_address', \n",
    "    'review_date', \n",
    "    'reviewer_nationality', \n",
    "    'tags'\n",
    "    ]\n",
    "\n",
    "df.drop(columns=colsToDrop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Processing\n",
    "- Cleaning\n",
    "- Normalization\n",
    "- Tokenization\n",
    "- Removing stopwords\n",
    "- POS tagging\n",
    "- Named Entity Recognition\n",
    "- Lemmatization\n",
    "\n",
    "2. Feature Extraction\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "- Word embeddings\n",
    "\n",
    "3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallTokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rooms stunningly decorated really spacious top building pictures room  true beauty building kept modernised brilliantly also bath lovely big inviting great couples restaurant menu bit pricey loads little eatery places nearby within walking distance tram stop centre  minute walk away   stops centre amsterdam would recommend hotel anyone unbelievably well priced'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate Spacy model for performing NER\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# apply model to text\n",
    "doc = nlp(df.positive_comments[10])\n",
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = list(nlp.pipe(df.positive_comments))\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities\n",
    "docEntities = list(doc.ents)\n",
    "docEntities\n",
    "\n",
    "overallTokens = overallTokens + docEntities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overallTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# !!! remove stopwords\\n\\n# nounPhrases\\nnounPhrases = [chunk.text for chunk in doc.noun_chunks]\\nnounPhrases\\n\\noverallTokens = overallTokens + nounPhrases\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# !!! remove stopwords\n",
    "\n",
    "# nounPhrases\n",
    "nounPhrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "nounPhrases\n",
    "\n",
    "overallTokens = overallTokens + nounPhrases\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['room', 'decorate', 'building', 'picture', 'room', 'beauty', 'building', 'keep', 'modernise', 'bath', 'invite', 'couple', 'restaurant', 'menu', 'bit', 'load', 'eatery', 'place', 'walk', 'distance', 'tram', 'stop', 'centre', 'minute', 'walk', 'stop', 'centre', 'amsterdam', 'recommend', 'hotel', 'price']\n"
     ]
    }
   ],
   "source": [
    "# !!! lemmatize first\n",
    "\n",
    "# using POS tagging\n",
    "\n",
    "pos = [\n",
    "    'PROPN',\n",
    "    'NOUN',\n",
    "    'VERB'\n",
    "]\n",
    "\n",
    "clean_doc = [token.lemma_ for token in doc if token.pos_ in pos]\n",
    "print(clean_doc)\n",
    "\n",
    "overallTokens = overallTokens + clean_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# sentences\n",
    "spSent = list(doc.sents)\n",
    "\n",
    "# words\n",
    "spWords = [w.text for w in doc]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'room': 2, 'building': 2, 'walk': 2, 'stop': 2, 'centre': 2, 'decorate': 1, 'picture': 1, 'beauty': 1, 'keep': 1, 'modernise': 1, 'bath': 1, 'invite': 1, 'couple': 1, 'restaurant': 1, 'menu': 1, 'bit': 1, 'load': 1, 'eatery': 1, 'place': 1, 'distance': 1, 'tram': 1, 'minute': 1, 'amsterdam': 1, 'recommend': 1, 'hotel': 1, 'price': 1})\n"
     ]
    }
   ],
   "source": [
    "# Word frequency\n",
    "from collections import Counter\n",
    "\n",
    "spCleanWordFreq = Counter(overallTokens)\n",
    "print(spCleanWordFreq)\n",
    "\n",
    "# counterSum = counter1 + counter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spCleanWordFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spCleanWordFreq.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room', 'building', 'walk', 'stop', 'centre']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Words occurring twice or more\n",
    "spFreqCleanWords = [w for (w, freq) in spCleanWordFreq.items() if freq > 1]\n",
    "spFreqCleanWords\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# dependency parsing\\nsentence = doc\\nspSentence = nlp(sentence)\\n\\nfor w in spSentence:\\n     print (w.text, w.tag_, w.head.text, w.dep_)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# dependency parsing\n",
    "sentence = doc\n",
    "spSentence = nlp(sentence)\n",
    "\n",
    "for w in spSentence:\n",
    "     print (w.text, w.tag_, w.head.text, w.dep_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# shallow parsing\\n# noun phrase detection\\n\\nfor chunk in doc.noun_chunks:\\n     print (chunk)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# shallow parsing\n",
    "# noun phrase detection\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "     print (chunk)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference resolution\n",
    "https://towardsdatascience.com/from-text-to-knowledge-the-information-extraction-pipeline-b65e7e30273e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-grams\n",
    "\n",
    "https://medium.com/analytics-vidhya/feature-extraction-and-sentiment-analysis-of-reviews-of-3-apps-in-india-84b665e1a887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# !pip install gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "my_docs = ['This movie was about spaceships and aliens.',\n",
    "           'I really enjoyed the movie!',\n",
    "           'Awesome action scenes, but boring characters.',\n",
    "           'The movie was awful! I hate alien films.',\n",
    "           'Space is cool! I like space movies.',\n",
    "           'More space films, please!']\n",
    "\n",
    "tokens = [] # list of lists >> column of pos/neg reviews tokens\n",
    "\n",
    "for doc in my_docs:\n",
    "    words = regexp_tokenize(doc.lower(), r'[A-Za-z]+')\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    tokens.append(words)\n",
    "\n",
    "my_dict = Dictionary(tokens) # dictionary\n",
    "print(my_dict.token2id) \n",
    "\n",
    "# Note: You can also add new words or lists of words to a dictionary, save a\n",
    "# dictionary to a file, load it back later, or read a dictionary from a text file\n",
    "\n",
    "# my_dict.add_documents(list_of_new_words)\n",
    "# my_dict.save('saved_dict.dict')\n",
    "# loaded_dict = Dictionary.load('saved_dict.dict')\n",
    "# dictionary = Dictionary(line.split()) for line in open('sample.txt', encoding='utf-8'))\n",
    "\n",
    "dtm = [my_dict.doc2bow(doc) for doc in tokens]\n",
    "dtm  # Create Gensim BOW corpus\n",
    "\n",
    "for doc in dtm:\n",
    "    print([[my_dict[i], freq] for i, freq in doc])\n",
    "\n",
    "tokenList = tokens # list of lists\n",
    "\n",
    "dtm = [my_dict.doc2bow(tokenListItem) for tokenListItem in tokenList]\n",
    "dtm  # Create Gensim BOW corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text  import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_df=0.85, stop_words=stopwords.words('english'))\n",
    "vec = CountVectorizer(max_df=0.85, max_features=5, stop_words='english')\n",
    "\n",
    "dtm = vec.fit_transform(df['positive_comments'])   # You can fit and transform jointly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0]\n",
      " [1 1 0 0 2]\n",
      " [1 1 0 0 1]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 1 0]\n",
      " [0 1 1 0 1]\n",
      " [0 0 0 0 0]\n",
      " [2 0 1 1 0]\n",
      " [1 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [2 1 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " [2 0 1 2 1]\n",
      " [0 1 1 0 0]\n",
      " [2 1 1 0 1]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [2 1 1 0 0]\n",
      " [1 1 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [3 1 0 3 1]\n",
      " [0 1 1 0 1]\n",
      " [4 1 1 2 1]\n",
      " [1 0 0 0 0]\n",
      " [1 0 1 0 0]\n",
      " [3 0 1 0 0]\n",
      " [0 0 1 0 1]\n",
      " [0 0 1 0 1]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 1]\n",
      " [0 1 1 1 0]\n",
      " [2 0 1 0 0]\n",
      " [0 0 1 1 0]\n",
      " [1 0 1 0 0]\n",
      " [0 1 0 1 1]\n",
      " [0 1 1 0 0]\n",
      " [1 0 1 0 1]\n",
      " [0 1 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 0]\n",
      " [1 1 0 1 1]\n",
      " [0 0 0 0 0]\n",
      " [1 1 0 1 0]\n",
      " [2 1 1 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 2 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 2 0 1]\n",
      " [0 0 2 1 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 1 0 1]\n",
      " [0 1 1 0 0]\n",
      " [1 1 1 2 1]\n",
      " [0 1 0 0 0]\n",
      " [3 1 0 1 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 3 2]\n",
      " [0 1 0 1 0]\n",
      " [2 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 3 1]\n",
      " [2 0 1 1 0]\n",
      " [0 0 0 1 1]\n",
      " [1 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [2 0 0 1 1]\n",
      " [0 1 1 0 0]\n",
      " [1 0 0 0 1]\n",
      " [3 0 0 2 1]\n",
      " [0 0 0 1 0]\n",
      " [1 1 0 0 0]\n",
      " [1 0 1 2 0]\n",
      " [0 1 0 0 0]\n",
      " [1 1 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 1]\n",
      " [1 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [2 1 0 1 1]\n",
      " [0 0 0 0 1]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 1 0]\n",
      " [2 1 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe9ce709e982859ebd8c1b094ee35d9f73a27801040ad55cc46450c9d5cadda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
