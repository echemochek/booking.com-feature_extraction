{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking.com Feature Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the file BookingDotCom_HotelReviews.xlsx from Canvas. This file contains over 515,000 guest reviews and rating of almost 1500 hotels across Europe scraped from popular hotel reservation website Booking.com. The text data was cleaned by removing unicode and punctuation and transformed to lower case. No other preprocessing was done. More information on each field is provided in the \"Data Description\" tab of the Excel file.\n",
    "\n",
    "        1. What are the top five hotel features (e.g., location, staff, etc.) that customers mention the most in positive reviews and top five features they mention most in negative reviews? Your identified features must make sense (e.g., \"great\" or \"negative\" are not features). (3 points)\n",
    "        \n",
    "        2. What are the top five features that customers prefer most if they are a solo traveler vs traveling with a group vs on a business trip vs a leisure trip vs traveling as a couple vs a family with young children. You will find these categories in the \"Tags\" column. There are a few more tags that we don't need. (2 points).\n",
    "\n",
    "        3. What are the top five features customers like most and top five features they complain about most about hotels in United Kingdom, France, Italy, and Spain? Country information is available inside Hotel_Address. (2 points)\n",
    "        \n",
    "        4. Create a dashboard with the following plots; (1) \"Top Five Hotels Overall\" with consistently high ratings, (2) Bottom Five Hotels Overall\" with consistently low ratings, (3) Five Most Improved Hotels\" with the highest improvement in average ratings from 2015 to 2017, showing their average ratings for each of the three years. (0.5+0.5+2 points).\n",
    "\n",
    "Write clear, compact, and understandable code with comment/markdown statements as appropriate. Non-working code or unnecessary code will be penalized. \n",
    "\n",
    "Submit your Jupyter file using the link below or provide a link to your Google Colab or Github file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages to use\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "df = pd.read_excel(\"BookingDotCom_HotelReviews.xlsx\", sheet_name=\"Data\")\n",
    "\n",
    "# sample the first 100 rows of df\n",
    "df = df[:100]\n",
    "\n",
    "# rename df columns to lower case\n",
    "df.columns= df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase & remove double/unneccessary spaces between words in columns of comments\n",
    "df['positive_comments'] = df['positive_comments'].str.lower().apply(lambda x: \" \".join(x.split()))\n",
    "df['negative_comments'] = df['negative_comments'].str.lower().apply(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create column with country name; we get the list of countries from the pycountry package\n",
    "and use the hotel_address column to extract country name\n",
    "'''\n",
    "\n",
    "df['country'] = df[\"hotel_address\"].apply(\n",
    "    lambda address: ' '.join([c.name for c in pycountry.countries if c.name in address])\n",
    "    )\n",
    "\n",
    "# use the review_date column to extract the year and store in new column\n",
    "df['year'] = pd.DatetimeIndex(df['review_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this step we deal with the tags column using the steps defined below:\n",
    "    1. Define tags we are interested in\n",
    "    2. Define a function to apply to tags column to remove tags we are not interested in by:\n",
    "            - Converting the individual row values to list (from string) e.g. \"[' Leisure trip ']\" -> [' leisure trip ']\n",
    "            - Strip the whitespaces from individual elements e.g. [' leisure trip ']-> ['leisure trip']\n",
    "            - Drop tags we are not interested in\n",
    "'''\n",
    "\n",
    "# customer tags we are interested in\n",
    "customer_tags = [\n",
    "    'solo traveler',\n",
    "    'group',\n",
    "    'business trip',\n",
    "    'leisure trip',\n",
    "    'couple',\n",
    "    'family with young children'\n",
    "]\n",
    "\n",
    "def clean_tag(x):\n",
    "\n",
    "    # convert value from string to a list\n",
    "    import ast\n",
    "    \n",
    "    myTags = ast.literal_eval(x.lower())\n",
    "\n",
    "    # strip whitespaces from elements and drop those we are not interested in\n",
    "    myTags = [customerTag.strip() for customerTag in myTags if customerTag.strip() in customer_tags]\n",
    "\n",
    "    return myTags\n",
    "\n",
    "\n",
    "# apply clean_tag() function to the tags column\n",
    "df['tags'] = df['tags'].apply(lambda x: clean_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function applied to tags column to extract new columns for customer categories\n",
    "Lambda function will be used as in the steps below\n",
    "'''\n",
    "def split_tag(x:list, tagName:str) -> int:\n",
    "    \n",
    "    t = [1 if tagName in x else 0][0]\n",
    "    \n",
    "    return t\n",
    "\n",
    "# dictionary of column names (new additional columns) and customer tags (as contained in tags of interest)\n",
    "tagDict = {\n",
    "    'solo_traveler' : 'solo traveler',\n",
    "    'group' : 'group',\n",
    "    'business_trip' : 'business trip',\n",
    "    'leisure_trip' : 'leisure trip',\n",
    "    'couple' : 'couple',\n",
    "    'family_with_young_children' : 'family with young children'\n",
    "}\n",
    "\n",
    "# applying function on tags column to get new separated columns\n",
    "for key, value in tagDict.items():\n",
    "    df[key] = df['tags'].apply(lambda x: split_tag(x, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we do not need for now\n",
    "colsToDrop = [\n",
    "    'hotel_address', \n",
    "    'review_date', \n",
    "    'reviewer_nationality', \n",
    "    'tags'\n",
    "    ]\n",
    "\n",
    "df.drop(columns=colsToDrop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Processing\n",
    "- Cleaning\n",
    "- Normalization\n",
    "- Tokenization\n",
    "- Removing stopwords\n",
    "- POS tagging\n",
    "- Named Entity Recognition\n",
    "- Lemmatization\n",
    "\n",
    "2. Feature Extraction\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "- Word embeddings\n",
    "\n",
    "3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rooms were stunningly decorated and really spacious in the top of the building pictures are of room 300 the true beauty of the building has been kept but modernised brilliantly also the bath was lovely and big and inviting great more for couples restaurant menu was a bit pricey but there were loads of little eatery places nearby within walking distance and the tram stop into the centre was about a 6 minute walk away and only about 3 or 4 stops from the centre of amsterdam would recommend this hotel to anyone it s unbelievably well priced too'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate Spacy model for performing NER\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# apply model to text\n",
    "doc = nlp(df.positive_comments[10])\n",
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rooms were stunningly decorated and really spacious in the top of the building pictures are of room 300 the true beauty of the building has been kept but modernised brilliantly also the bath was lovely and big and inviting great more for couples restaurant menu was a bit pricey but there were loads of little eatery places nearby within walking distance and the tram stop into the centre was about a 6 minute walk away and only about 3 or 4 stops from the centre of amsterdam would recommend this hotel to anyone it s unbelievably well priced too'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = list(nlp.pipe(df.positive_comments))\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 6 minute, only about 3, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entities\n",
    "docEntities = doc.ents\n",
    "docEntities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nounPhrases\n",
    "nounPhrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "nounPhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [\n",
    "    'PROPN',\n",
    "    'NOUN',\n",
    "    'VERB'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rooms, decorated, top, building, pictures, room, beauty, building, kept, modernised, bath, inviting, couples, restaurant, menu, bit, were, loads, eatery, places, walking, distance, tram, stop, centre, minute, walk, stops, centre, amsterdam, recommend, hotel, s, priced]\n"
     ]
    }
   ],
   "source": [
    "# using POS tagging\n",
    "clean_doc=[token for token in doc if token.pos_ in pos]\n",
    "print(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences\n",
    "spSent = list(doc.sents)\n",
    "\n",
    "# words\n",
    "spWords = [w.text for w in doc]\n",
    "\n",
    "# clean words: remove stopwords and punctuation\n",
    "spCleanWords = [w.text for w in doc\n",
    "           if not w.is_stop and not w.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency\n",
    "from collections import Counter\n",
    "spCleanWordFreq = Counter(spCleanWords)\n",
    "print(spCleanWordFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spCleanWordFreq.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words occurring twice or more\n",
    "spFreqCleanWords = [w for (w, freq) in spCleanWordFreq.items() if freq > 1]\n",
    "spFreqCleanWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "for w in doc:\n",
    "     print (w, w.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanSentence = ' '.join(spCleanWords).strip()\n",
    "CleanSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spCleanSentence = nlp(CleanSentence)\n",
    "spCleanWords = []\n",
    "\n",
    "for w in spCleanSentence:\n",
    "    spCleanWords.append(w.lemma_)\n",
    "\n",
    "print(spCleanWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting nouns using POS tags\n",
    "\n",
    "spNouns = []\n",
    "for w in doc:\n",
    "     if w.pos_ == 'NOUN':\n",
    "         spNouns.append(w)\n",
    "\n",
    "spNouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency parsing\n",
    "sentence = doc\n",
    "spSentence = nlp(sentence)\n",
    "\n",
    "for w in spSentence:\n",
    "     print (w.text, w.tag_, w.head.text, w.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shallow parsing\n",
    "for chunk in doc.noun_chunks:\n",
    "     print (chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True, options={'distance': 75})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun phrase detection\n",
    "for chunk in doc.noun_chunks:\n",
    "     print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference resolution\n",
    "https://towardsdatascience.com/from-text-to-knowledge-the-information-extraction-pipeline-b65e7e30273e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-grams\n",
    "\n",
    "https://medium.com/analytics-vidhya/feature-extraction-and-sentiment-analysis-of-reviews-of-3-apps-in-india-84b665e1a887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#pip install gensim\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_docs = ['This movie was about spaceships and aliens.',\n",
    "           'I really enjoyed the movie!',\n",
    "           'Awesome action scenes, but boring characters.',\n",
    "           'The movie was awful! I hate alien films.',\n",
    "           'Space is cool! I like space movies.',\n",
    "           'More space films, please!']\n",
    "\n",
    "tokens = [] # list of lists >> column of pos/neg reviews tokens\n",
    "for doc in my_docs:\n",
    "    words = regexp_tokenize(doc.lower(), r'[A-Za-z]+')\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    tokens.append(words)\n",
    "\n",
    "my_dict = Dictionary(tokens) # dictionary\n",
    "print(my_dict.token2id) \n",
    "\n",
    "# Note: You can also add new words or lists of words to a dictionary, save a\n",
    "# dictionary to a file, load it back later, or read a dictionary from a text file\n",
    "\n",
    "# my_dict.add_documents(list_of_new_words)\n",
    "# my_dict.save('saved_dict.dict')\n",
    "# loaded_dict = Dictionary.load('saved_dict.dict')\n",
    "# dictionary = Dictionary(line.split()) for line in open('sample.txt', encoding='utf-8'))\n",
    "\n",
    "dtm = [my_dict.doc2bow(doc) for doc in tokens]\n",
    "dtm  # Create Gensim BOW corpus\n",
    "\n",
    "for doc in dtm:\n",
    "    print([[my_dict[i], freq] for i, freq in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe9ce709e982859ebd8c1b094ee35d9f73a27801040ad55cc46450c9d5cadda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
